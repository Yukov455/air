"""
–°–µ—Ä–≤–∏—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏.
–ú–æ–¥–µ–ª—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–∞ –¥–ª—è –≤—Å–µ—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.
–í–∫–ª—é—á–∞–µ—Ç –≥–ª—É–±–æ–∫–∏–π NLP –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from loguru import logger
import sqlite3
import pickle
import json
import re
from pathlib import Path
from dataclasses import dataclass, asdict
from collections import Counter
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
import warnings
warnings.filterwarnings('ignore')

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π (–≤–ª–∏—è—é—Ç –Ω–∞ –∞–∫—Ü–∏–∏)
POSITIVE_KEYWORDS = [
    'growth', 'profit', 'revenue', 'beat', 'exceed', 'surge', 'rally', 'gain',
    'upgrade', 'buy', 'bullish', 'record', 'high', 'strong', 'success', 'deal',
    'partnership', 'innovation', 'breakthrough', 'expansion', 'dividend', 'buyback',
    '—Ä–æ—Å—Ç', '–ø—Ä–∏–±—ã–ª—å', '–≤—ã—Ä—É—á–∫–∞', '–ø—Ä–µ–≤—ã—Å–∏–ª', '—Ä–µ–∫–æ—Ä–¥', '—É—Å–ø–µ—Ö', '—Å–¥–µ–ª–∫–∞'
]

NEGATIVE_KEYWORDS = [
    'loss', 'decline', 'drop', 'fall', 'crash', 'miss', 'below', 'weak', 'concern',
    'downgrade', 'sell', 'bearish', 'low', 'fail', 'lawsuit', 'investigation',
    'layoff', 'cut', 'warning', 'risk', 'debt', 'default', 'bankruptcy', 'fraud',
    '–ø–∞–¥–µ–Ω–∏–µ', '—É–±—ã—Ç–æ–∫', '—Å–Ω–∏–∂–µ–Ω–∏–µ', '—Ä–∏—Å–∫', '–¥–æ–ª–≥', '–±–∞–Ω–∫—Ä–æ—Ç—Å—Ç–≤–æ', '–∏—Å–∫'
]

MARKET_EVENTS = [
    'fed', 'interest rate', 'inflation', 'gdp', 'unemployment', 'tariff', 'trade war',
    'recession', 'stimulus', 'quantitative', 'monetary', 'fiscal', 'regulation',
    '—Å–∞–Ω–∫—Ü–∏–∏', '–∏–Ω—Ñ–ª—è—Ü–∏—è', '—Å—Ç–∞–≤–∫–∞', '–±–µ–∑—Ä–∞–±–æ—Ç–∏—Ü–∞', '—Ä–µ—Ü–µ—Å—Å–∏—è'
]

SECTOR_KEYWORDS = {
    'tech': ['ai', 'artificial intelligence', 'cloud', 'software', 'chip', 'semiconductor', 'data'],
    'finance': ['bank', 'loan', 'credit', 'mortgage', 'insurance', 'investment'],
    'energy': ['oil', 'gas', 'renewable', 'solar', 'wind', 'battery', 'ev', 'electric'],
    'healthcare': ['drug', 'fda', 'clinical', 'trial', 'vaccine', 'pharma', 'biotech'],
    'retail': ['consumer', 'sales', 'store', 'e-commerce', 'amazon', 'walmart']
}

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
_training_progress = {
    'status': 'idle',
    'stage': '',
    'progress': 0,
    'message': '',
    'logs': [],
    'iteration': 0,
    'best_accuracy': 0,
    'target_accuracy': 0.95,
    'continuous_mode': False
}

# –§–ª–∞–≥ –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
_stop_continuous_training = False


@dataclass
class TrainingMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
    timestamp: str
    accuracy: float
    precision: float
    recall: float
    f1: float
    training_samples: int
    validation_samples: int
    feature_count: int
    model_version: str
    cross_val_mean: float
    cross_val_std: float


class UniversalModelTrainer:
    """
    –¢—Ä–µ–Ω–µ—Ä —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.
    
    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
    - –ï–¥–∏–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤
    - –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –¥–æ–æ–±—É—á–µ–Ω–∏–µ
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    - –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    """
    
    def __init__(self, db_path: str = "data/stock_analytics.db", 
                 model_path: str = "models/universal_model.pkl"):
        self.db_path = db_path
        self.model_path = model_path
        self.metrics_history = []
        
        # –ú–æ–¥–µ–ª–∏ (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å)
        self.rf_model = None
        self.gb_model = None
        self.et_model = None  # ExtraTrees
        self.mlp_model = None  # Neural Network
        self.ada_model = None  # AdaBoost
        self.scaler = StandardScaler()
        self.feature_names = []
        
        # –õ—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏
        self.best_accuracy = 0.0
        self.best_model_version = None
        
        # –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.model_version = "1.0.0"
        
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –µ—Å—Ç—å
        self._load_model()
        
        logger.info("UniversalModelTrainer initialized")
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏"""
        try:
            if Path(self.model_path).exists():
                with open(self.model_path, 'rb') as f:
                    data = pickle.load(f)
                    self.rf_model = data.get('rf_model')
                    self.gb_model = data.get('gb_model')
                    self.et_model = data.get('et_model')
                    self.mlp_model = data.get('mlp_model')
                    self.ada_model = data.get('ada_model')
                    self.scaler = data.get('scaler', StandardScaler())
                    self.feature_names = data.get('feature_names', [])
                    self.best_accuracy = data.get('best_accuracy', 0.0)
                    self.model_version = data.get('version', '1.0.0')
                    logger.info(f"Loaded model v{self.model_version} with accuracy {self.best_accuracy:.2%}")
        except Exception as e:
            logger.warning(f"Could not load model: {e}")
    
    def _save_model(self, metrics: TrainingMetrics):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        data = {
            'rf_model': self.rf_model,
            'gb_model': self.gb_model,
            'et_model': self.et_model,
            'mlp_model': self.mlp_model,
            'ada_model': self.ada_model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'best_accuracy': metrics.accuracy,
            'version': metrics.model_version,
            'trained_at': metrics.timestamp,
            'metrics': asdict(metrics)
        }
        
        with open(self.model_path, 'wb') as f:
            pickle.dump(data, f)
        
        logger.info(f"Model saved: v{metrics.model_version} with accuracy {metrics.accuracy:.2%}")
    
    def _save_metrics_to_db(self, metrics: TrainingMetrics):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ –ë–î"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO model_metrics 
            (timestamp, accuracy, precision_score, recall, f1_score, 
             training_samples, validation_samples, feature_count, model_version)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            metrics.timestamp, metrics.accuracy, metrics.precision,
            metrics.recall, metrics.f1, metrics.training_samples,
            metrics.validation_samples, metrics.feature_count, metrics.model_version
        ))
        
        conn.commit()
        conn.close()
    
    # ==================== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ====================
    
    def load_training_data(self, min_samples: int = 100) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ –ë–î.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ç–∏—Ä–æ–≤–∫–∏ –∏ –Ω–æ–≤–æ—Å—Ç–∏.
        """
        conn = sqlite3.connect(self.db_path)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ—Ç–∏—Ä–æ–≤–∫–∏
        quotes_df = pd.read_sql_query('''
            SELECT ticker, date, open, high, low, close, volume
            FROM quotes
            ORDER BY ticker, date
        ''', conn)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ (–∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü—ã)
        try:
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å –≤ —Ç–∞–±–ª–∏—Ü–µ
            cursor = conn.cursor()
            cursor.execute("PRAGMA table_info(news)")
            columns = {col[1] for col in cursor.fetchall()}
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫
            ticker_col = 'ticker' if 'ticker' in columns else 'tickers' if 'tickers' in columns else None
            summary_col = 'summary' if 'summary' in columns else 'description' if 'description' in columns else 'content' if 'content' in columns else None
            
            if ticker_col and summary_col:
                news_df = pd.read_sql_query(f'''
                    SELECT {ticker_col} as ticker, title, {summary_col} as summary, published_at
                    FROM news
                    ORDER BY published_at DESC
                ''', conn)
            else:
                # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å
                news_df = pd.read_sql_query('''
                    SELECT title, published_at FROM news ORDER BY published_at DESC
                ''', conn)
                news_df['ticker'] = 'GENERAL'
                news_df['summary'] = news_df['title']
            
            # –î–æ–±–∞–≤–ª—è–µ–º sentiment –µ—Å–ª–∏ –Ω–µ—Ç
            if 'sentiment' not in news_df.columns:
                news_df['sentiment'] = 0.0
                
            logger.info(f"News table columns: {columns}, loaded {len(news_df)} items")
        except Exception as e:
            logger.warning(f"Error loading news: {e}")
            news_df = pd.DataFrame(columns=['ticker', 'title', 'summary', 'sentiment', 'published_at'])
        
        conn.close()
        
        logger.info(f"Loaded {len(quotes_df)} quotes and {len(news_df)} news items")
        
        return quotes_df, news_df
    
    def prepare_features(self, quotes_df: pd.DataFrame, news_df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤—Å–µ—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤.
        """
        all_features = []
        all_targets = []
        
        for ticker in quotes_df['ticker'].unique():
            ticker_quotes = quotes_df[quotes_df['ticker'] == ticker].copy()
            ticker_quotes = ticker_quotes.sort_values('date').reset_index(drop=True)
            
            if len(ticker_quotes) < 20:
                logger.debug(f"Skipping {ticker}: only {len(ticker_quotes)} quotes")
                continue
            
            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            features = self._calculate_technical_features(ticker_quotes)
            
            # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            ticker_news = news_df[news_df['ticker'] == ticker]
            news_features = self._calculate_news_features(ticker_quotes, ticker_news)
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º
            features = pd.concat([features, news_features], axis=1)
            
            # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è (—Ä–æ—Å—Ç/–ø–∞–¥–µ–Ω–∏–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –¥–µ–Ω—å)
            target = (ticker_quotes['close'].shift(-1) > ticker_quotes['close']).astype(int)
            
            # –£–±–∏—Ä–∞–µ–º NaN
            valid_idx = features.dropna().index
            valid_idx = valid_idx[valid_idx < len(target) - 1]  # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å
            
            if len(valid_idx) >= 5:
                all_features.append(features.loc[valid_idx])
                all_targets.append(target.loc[valid_idx])
                logger.debug(f"Added {len(valid_idx)} samples for {ticker}")
        
        if not all_features:
            return pd.DataFrame(), pd.Series()
        
        X = pd.concat(all_features, ignore_index=True)
        y = pd.concat(all_targets, ignore_index=True)
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è NaN
        X = X.fillna(0)
        
        self.feature_names = list(X.columns)
        
        logger.info(f"Prepared {len(X)} samples with {len(self.feature_names)} features")
        
        return X, y
    
    def _calculate_technical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–†–∞—Å—á—ë—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤"""
        features = pd.DataFrame(index=df.index)
        
        close = df['close']
        high = df['high']
        low = df['low']
        volume = df['volume']
        
        # Returns
        features['return_1d'] = close.pct_change(1)
        features['return_5d'] = close.pct_change(5)
        features['return_10d'] = close.pct_change(10)
        features['return_20d'] = close.pct_change(20)
        
        # Moving averages (–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –ø–µ—Ä–∏–æ–¥—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–ª–∏–Ω—ã –¥–∞–Ω–Ω—ã—Ö)
        n = len(close)
        periods = [5, 10, 20]
        if n >= 50:
            periods.append(50)
        
        for period in periods:
            ma = close.rolling(min(period, n-1)).mean()
            features[f'ma_{period}'] = ma
            features[f'ma_{period}_ratio'] = close / ma.replace(0, 1e-10)
        
        # –ï—Å–ª–∏ MA_50 –Ω–µ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º MA_20
        if 'ma_50' not in features.columns:
            features['ma_50'] = features['ma_20']
            features['ma_50_ratio'] = features['ma_20_ratio']
        
        # Volatility (—Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –ø–µ—Ä–∏–æ–¥–æ–º 2)
        pct_change = close.pct_change()
        features['volatility_5d'] = pct_change.rolling(min(5, n-1), min_periods=2).std()
        features['volatility_10d'] = pct_change.rolling(min(10, n-1), min_periods=2).std()
        features['volatility_20d'] = pct_change.rolling(min(20, n-1), min_periods=2).std()
        
        # RSI
        delta = close.diff()
        gain = delta.where(delta > 0, 0).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / loss.replace(0, 1e-10)
        features['rsi'] = 100 - (100 / (1 + rs))
        
        # MACD
        ema12 = close.ewm(span=12).mean()
        ema26 = close.ewm(span=26).mean()
        features['macd'] = ema12 - ema26
        features['macd_signal'] = features['macd'].ewm(span=9).mean()
        features['macd_hist'] = features['macd'] - features['macd_signal']
        
        # Bollinger Bands
        ma20 = close.rolling(20).mean()
        std20 = close.rolling(20).std()
        features['bb_upper'] = ma20 + 2 * std20
        features['bb_lower'] = ma20 - 2 * std20
        features['bb_position'] = (close - features['bb_lower']) / (features['bb_upper'] - features['bb_lower'])
        
        # Volume features
        features['volume_ma_ratio'] = volume / volume.rolling(20).mean()
        features['volume_change'] = volume.pct_change()
        
        # Price patterns
        features['high_low_ratio'] = high / low
        features['close_position'] = (close - low) / (high - low + 1e-10)
        
        # Momentum
        features['momentum_5'] = close / close.shift(5) - 1
        features['momentum_10'] = close / close.shift(10) - 1
        features['momentum_20'] = close / close.shift(20) - 1
        
        # Trend strength
        features['trend_strength'] = (close - close.rolling(20).min()) / (close.rolling(20).max() - close.rolling(20).min() + 1e-10)
        
        return features
    
    def _calculate_news_features(self, quotes_df: pd.DataFrame, news_df: pd.DataFrame) -> pd.DataFrame:
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π NLP –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π.
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ sentiment, –Ω–æ –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞.
        """
        features = pd.DataFrame(index=quotes_df.index)
        
        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['news_sentiment_mean'] = 0.0
        features['news_sentiment_std'] = 0.0
        features['news_count'] = 0
        
        # NLP –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['positive_keywords'] = 0
        features['negative_keywords'] = 0
        features['market_events'] = 0
        features['keyword_ratio'] = 0.0
        
        # –°–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features['tech_mentions'] = 0
        features['finance_mentions'] = 0
        features['energy_mentions'] = 0
        features['healthcare_mentions'] = 0
        features['retail_mentions'] = 0
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –Ω–æ–≤–æ—Å—Ç–µ–π
        features['news_urgency'] = 0.0
        features['news_impact_score'] = 0.0
        
        if news_df.empty:
            return features
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—ã –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞—Ä–∞–Ω–µ–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        news_df = news_df.copy()
        # –ü–∞—Ä—Å–∏–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç (–≤–∫–ª—é—á–∞—è 20260104T232031)
        def parse_news_date(date_str):
            try:
                if isinstance(date_str, str):
                    # –§–æ—Ä–º–∞—Ç 20260104T232031
                    if 'T' in date_str and len(date_str) == 15:
                        return pd.to_datetime(date_str, format='%Y%m%dT%H%M%S')
                return pd.to_datetime(date_str)
            except:
                return pd.NaT
        news_df['parsed_date'] = news_df['published_at'].apply(parse_news_date)
        
        for idx, row in quotes_df.iterrows():
            date = row['date']
            
            try:
                date_dt = pd.to_datetime(date)
                recent_news = news_df[
                    (news_df['parsed_date'] >= date_dt - timedelta(days=3)) &
                    (news_df['parsed_date'] <= date_dt + timedelta(days=1))
                ]
                
                if len(recent_news) > 0:
                    # –ë–∞–∑–æ–≤—ã–π sentiment
                    features.loc[idx, 'news_sentiment_mean'] = recent_news['sentiment'].mean()
                    features.loc[idx, 'news_sentiment_std'] = recent_news['sentiment'].std() if len(recent_news) > 1 else 0
                    features.loc[idx, 'news_count'] = len(recent_news)
                    
                    # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π - —Å–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
                    titles = ' '.join(recent_news['title'].fillna('').astype(str).tolist())
                    summaries = ' '.join(recent_news['summary'].fillna('').astype(str).tolist()) if 'summary' in recent_news.columns else ''
                    all_text = (titles + ' ' + summaries).lower()
                    
                    # –ü–æ–¥—Å—á—ë—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                    pos_count = sum(1 for kw in POSITIVE_KEYWORDS if kw in all_text)
                    neg_count = sum(1 for kw in NEGATIVE_KEYWORDS if kw in all_text)
                    market_count = sum(1 for kw in MARKET_EVENTS if kw in all_text)
                    
                    features.loc[idx, 'positive_keywords'] = pos_count
                    features.loc[idx, 'negative_keywords'] = neg_count
                    features.loc[idx, 'market_events'] = market_count
                    
                    # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö/–Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö
                    total_kw = pos_count + neg_count
                    if total_kw > 0:
                        features.loc[idx, 'keyword_ratio'] = (pos_count - neg_count) / total_kw
                    
                    # –°–µ–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                    for sector, keywords in SECTOR_KEYWORDS.items():
                        count = sum(1 for kw in keywords if kw in all_text)
                        features.loc[idx, f'{sector}_mentions'] = count
                    
                    # –°—Ä–æ—á–Ω–æ—Å—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ (–Ω–∞–ª–∏—á–∏–µ —Å—Ä–æ—á–Ω—ã—Ö —Å–ª–æ–≤)
                    urgency_words = ['breaking', 'urgent', 'alert', 'just in', '—Å—Ä–æ—á–Ω–æ', '–≤–∞–∂–Ω–æ']
                    urgency = sum(1 for w in urgency_words if w in all_text)
                    features.loc[idx, 'news_urgency'] = min(urgency, 5) / 5.0
                    
                    # –û–±—â–∏–π impact score
                    impact = (pos_count * 0.3 - neg_count * 0.4 + market_count * 0.2 + 
                              features.loc[idx, 'news_sentiment_mean'] * 0.5)
                    features.loc[idx, 'news_impact_score'] = np.clip(impact, -1, 1)
                    
            except Exception as e:
                pass
        
        return features
    
    # ==================== –û–ë–£–ß–ï–ù–ò–ï ====================
    
    def _update_progress(self, status: str, stage: str, progress: int, message: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        global _training_progress
        _training_progress['status'] = status
        _training_progress['stage'] = stage
        _training_progress['progress'] = progress
        _training_progress['message'] = message
        _training_progress['logs'].append(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")
        # –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 50 –ª–æ–≥–æ–≤
        if len(_training_progress['logs']) > 50:
            _training_progress['logs'] = _training_progress['logs'][-50:]
        logger.info(f"[TRAINING] {message}")
    
    def train(self, force: bool = False) -> Optional[TrainingMetrics]:
        """
        –û–±—É—á–µ–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π.
        
        –ú–æ–¥–µ–ª–∏:
        - RandomForest (200 –¥–µ—Ä–µ–≤—å–µ–≤)
        - GradientBoosting (200 –∏—Ç–µ—Ä–∞—Ü–∏–π)
        - ExtraTrees (200 –¥–µ—Ä–µ–≤—å–µ–≤)
        - Neural Network (3 —Å–ª–æ—è)
        - AdaBoost (100 –∏—Ç–µ—Ä–∞—Ü–∏–π)
        
        Args:
            force: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –¥–∞–∂–µ –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–∞–ª–æ
        
        Returns:
            –ú–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
        """
        global _training_progress
        _training_progress['logs'] = []
        
        self._update_progress('running', 'init', 0, 'üöÄ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...')
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        self._update_progress('running', 'loading', 5, 'üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ë–î...')
        quotes_df, news_df = self.load_training_data()
        
        if quotes_df.empty:
            self._update_progress('error', 'loading', 0, '‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è')
            return None
        
        self._update_progress('running', 'loading', 10, f'‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(quotes_df)} –∫–æ—Ç–∏—Ä–æ–≤–æ–∫, {len(news_df)} –Ω–æ–≤–æ—Å—Ç–µ–π')
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        self._update_progress('running', 'features', 15, 'üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...')
        X, y = self.prepare_features(quotes_df, news_df)
        
        if len(X) < 50 and not force:
            self._update_progress('error', 'features', 0, f'‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö ({len(X)} < 50)')
            return None
        
        self._update_progress('running', 'features', 20, f'‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤ —Å {len(self.feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏')
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, shuffle=False
        )
        
        self._update_progress('running', 'scaling', 22, f'üìê –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö (train: {len(X_train)}, test: {len(X_test)})...')
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # ==================== –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ====================
        
        # 1. RandomForest (—É–ª—É—á—à–µ–Ω–Ω—ã–π)
        self._update_progress('running', 'rf', 25, 'üå≤ [1/5] –û–±—É—á–µ–Ω–∏–µ RandomForest (200 –¥–µ—Ä–µ–≤—å–µ–≤)...')
        self.rf_model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=3,
            min_samples_leaf=2,
            max_features='sqrt',
            random_state=42,
            n_jobs=-1
        )
        self.rf_model.fit(X_train_scaled, y_train)
        rf_acc = accuracy_score(y_test, self.rf_model.predict(X_test_scaled))
        self._update_progress('running', 'rf', 35, f'‚úÖ RandomForest: accuracy={rf_acc:.2%}')
        
        # 2. GradientBoosting (—É–ª—É—á—à–µ–Ω–Ω—ã–π)
        self._update_progress('running', 'gb', 40, 'üìà [2/5] –û–±—É—á–µ–Ω–∏–µ GradientBoosting (200 –∏—Ç–µ—Ä–∞—Ü–∏–π)...')
        self.gb_model = GradientBoostingClassifier(
            n_estimators=200,
            max_depth=7,
            learning_rate=0.05,
            min_samples_split=3,
            min_samples_leaf=2,
            subsample=0.8,
            random_state=42
        )
        self.gb_model.fit(X_train_scaled, y_train)
        gb_acc = accuracy_score(y_test, self.gb_model.predict(X_test_scaled))
        self._update_progress('running', 'gb', 50, f'‚úÖ GradientBoosting: accuracy={gb_acc:.2%}')
        
        # 3. ExtraTrees
        self._update_progress('running', 'et', 55, 'üå≥ [3/5] –û–±—É—á–µ–Ω–∏–µ ExtraTrees (200 –¥–µ—Ä–µ–≤—å–µ–≤)...')
        self.et_model = ExtraTreesClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=3,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        self.et_model.fit(X_train_scaled, y_train)
        et_acc = accuracy_score(y_test, self.et_model.predict(X_test_scaled))
        self._update_progress('running', 'et', 65, f'‚úÖ ExtraTrees: accuracy={et_acc:.2%}')
        
        # 4. Neural Network (MLP)
        self._update_progress('running', 'mlp', 70, 'üß† [4/5] –û–±—É—á–µ–Ω–∏–µ Neural Network (3 —Å–ª–æ—è: 128-64-32)...')
        self.mlp_model = MLPClassifier(
            hidden_layer_sizes=(128, 64, 32),
            activation='relu',
            solver='adam',
            alpha=0.001,
            batch_size='auto',
            learning_rate='adaptive',
            learning_rate_init=0.001,
            max_iter=500,
            early_stopping=True,
            validation_fraction=0.1,
            random_state=42
        )
        self.mlp_model.fit(X_train_scaled, y_train)
        mlp_acc = accuracy_score(y_test, self.mlp_model.predict(X_test_scaled))
        self._update_progress('running', 'mlp', 80, f'‚úÖ Neural Network: accuracy={mlp_acc:.2%}')
        
        # 5. AdaBoost
        self._update_progress('running', 'ada', 85, 'üéØ [5/5] –û–±—É—á–µ–Ω–∏–µ AdaBoost (100 –∏—Ç–µ—Ä–∞—Ü–∏–π)...')
        self.ada_model = AdaBoostClassifier(
            n_estimators=100,
            learning_rate=0.1,
            random_state=42
        )
        self.ada_model.fit(X_train_scaled, y_train)
        ada_acc = accuracy_score(y_test, self.ada_model.predict(X_test_scaled))
        self._update_progress('running', 'ada', 90, f'‚úÖ AdaBoost: accuracy={ada_acc:.2%}')
        
        # ==================== –ê–ù–°–ê–ú–ë–õ–¨ ====================
        self._update_progress('running', 'ensemble', 92, 'üîó –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –∏–∑ 5 –º–æ–¥–µ–ª–µ–π...')
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        rf_pred = self.rf_model.predict_proba(X_test_scaled)[:, 1]
        gb_pred = self.gb_model.predict_proba(X_test_scaled)[:, 1]
        et_pred = self.et_model.predict_proba(X_test_scaled)[:, 1]
        mlp_pred = self.mlp_model.predict_proba(X_test_scaled)[:, 1]
        ada_pred = self.ada_model.predict_proba(X_test_scaled)[:, 1]
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ (–ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç –±–æ–ª—å—à–∏–π –≤–µ—Å)
        weights = np.array([rf_acc, gb_acc, et_acc, mlp_acc, ada_acc])
        weights = weights / weights.sum()  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        
        ensemble_proba = (
            weights[0] * rf_pred + 
            weights[1] * gb_pred + 
            weights[2] * et_pred + 
            weights[3] * mlp_pred + 
            weights[4] * ada_pred
        )
        y_pred = (ensemble_proba > 0.5).astype(int)
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∞–Ω—Å–∞–º–±–ª—è
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        self._update_progress('running', 'ensemble', 95, f'üìä –ê–Ω—Å–∞–º–±–ª—å: accuracy={accuracy:.2%}, F1={f1:.2%}')
        
        # Cross-validation
        self._update_progress('running', 'cv', 97, 'üîÑ –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è...')
        cv_scores = cross_val_score(self.rf_model, X_train_scaled, y_train, cv=5)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Ä—Å–∏—é
        version_parts = self.model_version.split('.')
        version_parts[-1] = str(int(version_parts[-1]) + 1)
        new_version = '.'.join(version_parts)
        
        metrics = TrainingMetrics(
            timestamp=datetime.now().isoformat(),
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1=f1,
            training_samples=len(X_train),
            validation_samples=len(X_test),
            feature_count=len(self.feature_names),
            model_version=new_version,
            cross_val_mean=cv_scores.mean(),
            cross_val_std=cv_scores.std()
        )
        
        self._update_progress('running', 'saving', 98, f'üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ v{new_version}...')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ—Å–ª–∏ –ª—É—á—à–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–π
        if accuracy >= self.best_accuracy or force:
            self.model_version = new_version
            self.best_accuracy = accuracy
            self._save_model(metrics)
            self._save_metrics_to_db(metrics)
            self._update_progress('completed', 'done', 100, f'üéâ –ú–æ–¥–µ–ª—å v{new_version} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞! Accuracy: {accuracy:.2%}')
        else:
            self._update_progress('completed', 'done', 100, f'‚ÑπÔ∏è –ú–æ–¥–µ–ª—å –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (accuracy {accuracy:.2%} < best {self.best_accuracy:.2%})')
        
        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info(f"=== TRAINING SUMMARY ===")
        logger.info(f"RandomForest: {rf_acc:.2%}")
        logger.info(f"GradientBoosting: {gb_acc:.2%}")
        logger.info(f"ExtraTrees: {et_acc:.2%}")
        logger.info(f"Neural Network: {mlp_acc:.2%}")
        logger.info(f"AdaBoost: {ada_acc:.2%}")
        logger.info(f"ENSEMBLE: {accuracy:.2%} (F1: {f1:.2%})")
        logger.info(f"Cross-validation: {cv_scores.mean():.2%} (+/- {cv_scores.std()*2:.2%})")
        
        self.metrics_history.append(metrics)
        
        return metrics
    
    def train_continuous(self, target_accuracy: float = 0.95, max_iterations: int = 0, 
                         data_refresh_interval: int = 5) -> Optional[TrainingMetrics]:
        """
        –ë–ï–°–ö–û–ù–ï–ß–ù–û–ï –æ–±—É—á–µ–Ω–∏–µ –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏.
        –ú–æ–¥–µ–ª—å —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —É–º–Ω–µ–µ —Å –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–µ–π –±–ª–∞–≥–æ–¥–∞—Ä—è:
        - –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        - –ü–æ—Å—Ç–æ—è–Ω–Ω–æ–π –ø–æ–¥–≥—Ä—É–∑–∫–µ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        - NLP –∞–Ω–∞–ª–∏–∑—É –Ω–æ–≤–æ—Å—Ç–µ–π
        - –≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–º—É –ø–æ–¥–±–æ—Ä—É –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        
        Args:
            target_accuracy: –¶–µ–ª–µ–≤–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 95%)
            max_iterations: 0 = –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ, –∏–Ω–∞—á–µ –ª–∏–º–∏—Ç –∏—Ç–µ—Ä–∞—Ü–∏–π
            data_refresh_interval: –ò–Ω—Ç–µ—Ä–≤–∞–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (–∫–∞–∂–¥—ã–µ N –∏—Ç–µ—Ä–∞—Ü–∏–π)
        
        Returns:
            –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–ª–∏ None
        """
        global _training_progress, _stop_continuous_training
        _stop_continuous_training = False
        
        _training_progress['continuous_mode'] = True
        _training_progress['target_accuracy'] = target_accuracy
        _training_progress['iteration'] = 0
        _training_progress['best_accuracy'] = self.best_accuracy
        _training_progress['logs'] = []
        
        # –ò—Å—Ç–æ—Ä–∏—è –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —ç–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.best_params_history = []
        self.accuracy_history = []
        
        mode = "‚ôæÔ∏è –ë–ï–°–ö–û–ù–ï–ß–ù–û–ï" if max_iterations == 0 else f"–¥–æ {max_iterations} –∏—Ç–µ—Ä–∞—Ü–∏–π"
        self._update_progress('running', 'continuous_init', 0, 
            f'üöÄ –ó–∞–ø—É—Å–∫ {mode} –æ–±—É—á–µ–Ω–∏—è –¥–æ {target_accuracy:.0%} —Ç–æ—á–Ω–æ—Å—Ç–∏...')
        self._update_progress('running', 'continuous_init', 0, 
            f'üß† NLP –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π –í–ö–õ–Æ–ß–ï–ù - –º–æ–¥–µ–ª—å –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç')
        
        best_metrics = None
        iteration = 0
        no_improvement_count = 0
        
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º data_updater –¥–ª—è –ø–æ–¥–≥—Ä—É–∑–∫–∏ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        from services.data_updater import get_data_updater
        data_updater = get_data_updater()
        
        import time
        
        # –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª (–∏–ª–∏ –¥–æ max_iterations –µ—Å–ª–∏ –∑–∞–¥–∞–Ω)
        while not _stop_continuous_training:
            iteration += 1
            _training_progress['iteration'] = iteration
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞ –∏—Ç–µ—Ä–∞—Ü–∏–π (0 = –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ)
            if max_iterations > 0 and iteration > max_iterations:
                break
            
            # –ü—Ä–æ–≥—Ä–µ—Å—Å (–¥–ª—è –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å)
            progress = min(int(self.best_accuracy * 100), 99) if max_iterations == 0 else int((iteration / max_iterations) * 100)
            
            self._update_progress('running', 'iteration', progress,
                f'üìä –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration} | –õ—É—á—à–∞—è: {self.best_accuracy:.2%} | –¶–µ–ª—å: {target_accuracy:.0%}')
            
            # –ü–æ–¥–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥—ã–µ N –∏—Ç–µ—Ä–∞—Ü–∏–π
            if iteration % data_refresh_interval == 1:
                self._update_progress('running', 'data_refresh', progress,
                    f'üîÑ –ü–æ–¥–≥—Ä—É–∑–∫–∞ —Å–≤–µ–∂–∏—Ö –¥–∞–Ω–Ω—ã—Ö (–∏—Ç–µ—Ä–∞—Ü–∏—è {iteration})...')
                try:
                    stats = data_updater.update_all()
                    self._update_progress('running', 'data_refresh', progress,
                        f'‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {stats.quotes_updated} –∫–æ—Ç–∏—Ä–æ–≤–æ–∫, {stats.news_updated} –Ω–æ–≤–æ—Å—Ç–µ–π')
                except Exception as e:
                    self._update_progress('running', 'data_refresh', progress,
                        f'‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)[:40]}')
            
            # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
            try:
                metrics = self._train_adaptive_iteration(iteration, no_improvement_count)
                
                if metrics:
                    self.accuracy_history.append(metrics.accuracy)
                    _training_progress['best_accuracy'] = self.best_accuracy
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–∏
                    if metrics.accuracy >= target_accuracy:
                        self._update_progress('completed', 'target_reached', 100,
                            f'üéâüéâüéâ –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê! Accuracy: {metrics.accuracy:.2%} >= {target_accuracy:.0%}')
                        self._update_progress('completed', 'target_reached', 100,
                            f'üèÜ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∑–∞ {iteration} –∏—Ç–µ—Ä–∞—Ü–∏–π!')
                        _training_progress['continuous_mode'] = False
                        return metrics
                    
                    # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π
                    if metrics.accuracy > (best_metrics.accuracy if best_metrics else 0):
                        best_metrics = metrics
                        no_improvement_count = 0
                        self._update_progress('running', 'new_best', progress,
                            f'üèÜ –ù–û–í–´–ô –†–ï–ö–û–†–î: {metrics.accuracy:.2%} (–∏—Ç–µ—Ä–∞—Ü–∏—è {iteration})')
                    else:
                        no_improvement_count += 1
                        
                    # –ï—Å–ª–∏ –¥–æ–ª–≥–æ –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π - –º–µ–Ω—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
                    if no_improvement_count >= 10:
                        self._update_progress('running', 'strategy_change', progress,
                            f'üîÄ –°–º–µ–Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–±—É—á–µ–Ω–∏—è (–Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π {no_improvement_count} –∏—Ç–µ—Ä–∞—Ü–∏–π)')
                        no_improvement_count = 0
                    
            except Exception as e:
                self._update_progress('running', 'error', progress,
                    f'‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}: {str(e)[:40]}')
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∏—Ç–µ—Ä–∞—Ü–∏—è–º–∏ (–∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è)
            sleep_time = 0.5 if no_improvement_count < 5 else 1
            time.sleep(sleep_time)
        
        if _stop_continuous_training:
            self._update_progress('stopped', 'user_stopped', progress,
                f'‚èπÔ∏è –û–±—É—á–µ–Ω–∏–µ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏–∏ {iteration}. –õ—É—á—à–∞—è: {self.best_accuracy:.2%}')
        else:
            self._update_progress('completed', 'max_iterations', 100,
                f'‚è±Ô∏è –õ–∏–º–∏—Ç –∏—Ç–µ—Ä–∞—Ü–∏–π ({max_iterations}). –õ—É—á—à–∞—è: {self.best_accuracy:.2%}')
        
        _training_progress['continuous_mode'] = False
        return best_metrics
    
    def _train_adaptive_iteration(self, iteration: int, no_improvement: int) -> Optional[TrainingMetrics]:
        """
        –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –∏—Ç–µ—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è.
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        """
        global _training_progress
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        quotes_df, news_df = self.load_training_data()
        
        if quotes_df.empty:
            return None
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ (—Ç–µ–ø–µ—Ä—å —Å NLP)
        X, y = self.prepare_features(quotes_df, news_df)
        
        if len(X) < 50:
            return None
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        test_size = 0.15 + (iteration % 10) * 0.01  # 0.15-0.24
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=42 + iteration, shuffle=True
        )
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # –≠–í–û–õ–Æ–¶–ò–û–ù–ù–´–ï –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã - —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –ª—É—á—à–µ —Å –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–µ–π
        # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–∞—Å—Ç—É—Ç —Å –∏—Ç–µ—Ä–∞—Ü–∏—è–º–∏
        base_estimators = min(200 + iteration * 10, 1000)  # 200 -> 1000
        base_depth = min(10 + iteration // 5, 30)  # 10 -> 30
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        n_estimators = base_estimators + np.random.randint(-50, 50)
        max_depth = base_depth + np.random.randint(-2, 3)
        learning_rate = 0.01 + np.random.random() * 0.09  # 0.01-0.1
        
        # –ï—Å–ª–∏ –¥–æ–ª–≥–æ –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π - —Ä–∞–¥–∏–∫–∞–ª—å–Ω–æ –º–µ–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if no_improvement >= 5:
            n_estimators = np.random.randint(100, 500)
            max_depth = np.random.randint(5, 25)
            learning_rate = np.random.random() * 0.2
        
        self._update_progress('running', f'training_{iteration}', 
            min(int(self.best_accuracy * 100), 99),
            f'üß¨ Iter {iteration}: est={n_estimators}, depth={max_depth}, lr={learning_rate:.3f}')
        
        # –û–±—É—á–∞–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
        # 1. RandomForest
        self.rf_model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=max(2, 5 - iteration // 20),
            min_samples_leaf=max(1, 3 - iteration // 30),
            max_features='sqrt',
            random_state=42 + iteration,
            n_jobs=-1
        )
        self.rf_model.fit(X_train_scaled, y_train)
        rf_acc = accuracy_score(y_test, self.rf_model.predict(X_test_scaled))
        
        # 2. GradientBoosting
        self.gb_model = GradientBoostingClassifier(
            n_estimators=n_estimators,
            max_depth=max(3, max_depth // 2),
            learning_rate=learning_rate,
            subsample=0.7 + np.random.random() * 0.2,
            random_state=42 + iteration
        )
        self.gb_model.fit(X_train_scaled, y_train)
        gb_acc = accuracy_score(y_test, self.gb_model.predict(X_test_scaled))
        
        # 3. ExtraTrees
        self.et_model = ExtraTreesClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42 + iteration,
            n_jobs=-1
        )
        self.et_model.fit(X_train_scaled, y_train)
        et_acc = accuracy_score(y_test, self.et_model.predict(X_test_scaled))
        
        # 4. Neural Network - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç
        layers = self._get_evolved_nn_architecture(iteration, no_improvement)
        self.mlp_model = MLPClassifier(
            hidden_layer_sizes=layers,
            activation='relu',
            solver='adam',
            alpha=0.0001 * (1 + iteration % 10),
            learning_rate='adaptive',
            max_iter=500 + iteration * 20,
            early_stopping=True,
            random_state=42 + iteration
        )
        self.mlp_model.fit(X_train_scaled, y_train)
        mlp_acc = accuracy_score(y_test, self.mlp_model.predict(X_test_scaled))
        
        # 5. AdaBoost
        self.ada_model = AdaBoostClassifier(
            n_estimators=min(100 + iteration * 5, 500),
            learning_rate=learning_rate,
            random_state=42 + iteration
        )
        self.ada_model.fit(X_train_scaled, y_train)
        ada_acc = accuracy_score(y_test, self.ada_model.predict(X_test_scaled))
        
        # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å
        accuracies = np.array([rf_acc, gb_acc, et_acc, mlp_acc, ada_acc])
        weights = accuracies / accuracies.sum()
        
        rf_pred = self.rf_model.predict_proba(X_test_scaled)[:, 1]
        gb_pred = self.gb_model.predict_proba(X_test_scaled)[:, 1]
        et_pred = self.et_model.predict_proba(X_test_scaled)[:, 1]
        mlp_pred = self.mlp_model.predict_proba(X_test_scaled)[:, 1]
        ada_pred = self.ada_model.predict_proba(X_test_scaled)[:, 1]
        
        ensemble_proba = (weights[0] * rf_pred + weights[1] * gb_pred + 
                         weights[2] * et_pred + weights[3] * mlp_pred + weights[4] * ada_pred)
        y_pred = (ensemble_proba > 0.5).astype(int)
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self._update_progress('running', f'results_{iteration}', 
            min(int(self.best_accuracy * 100), 99),
            f'üìä RF={rf_acc:.1%} GB={gb_acc:.1%} ET={et_acc:.1%} NN={mlp_acc:.1%} ADA={ada_acc:.1%} ‚Üí {accuracy:.2%}')
        
        # –í–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏
        version_parts = self.model_version.split('.')
        version_parts[-1] = str(int(version_parts[-1]) + 1)
        new_version = '.'.join(version_parts)
        
        metrics = TrainingMetrics(
            timestamp=datetime.now().isoformat(),
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1=f1,
            training_samples=len(X_train),
            validation_samples=len(X_test),
            feature_count=len(self.feature_names),
            model_version=new_version,
            cross_val_mean=accuracy,
            cross_val_std=0.0
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ—Å–ª–∏ –ª—É—á—à–µ
        if accuracy > self.best_accuracy:
            self.model_version = new_version
            self.best_accuracy = accuracy
            self._save_model(metrics)
            self._save_metrics_to_db(metrics)
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            self.best_params_history.append({
                'n_estimators': n_estimators,
                'max_depth': max_depth,
                'learning_rate': learning_rate,
                'accuracy': accuracy
            })
        
        return metrics
    
    def _get_evolved_nn_architecture(self, iteration: int, no_improvement: int) -> Tuple:
        """–≠–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
        # –ë–∞–∑–æ–≤—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        architectures = [
            (128, 64, 32),
            (256, 128, 64),
            (256, 128, 64, 32),
            (512, 256, 128),
            (512, 256, 128, 64),
            (256, 256, 128, 64),
            (512, 512, 256, 128),
            (1024, 512, 256, 128),
        ]
        
        # –° —Ä–æ—Å—Ç–æ–º –∏—Ç–µ—Ä–∞—Ü–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
        idx = min(iteration // 10, len(architectures) - 1)
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —É–ª—É—á—à–µ–Ω–∏–π - –ø—Ä–æ–±—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
        if no_improvement >= 5:
            idx = np.random.randint(0, len(architectures))
        
        return architectures[idx]
    
    def _train_single_iteration(self, iteration: int) -> Optional[TrainingMetrics]:
        """–û–¥–Ω–∞ –∏—Ç–µ—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è —Å –≤–∞—Ä–∏–∞—Ü–∏–µ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        global _training_progress
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        quotes_df, news_df = self.load_training_data()
        
        if quotes_df.empty:
            return None
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        X, y = self.prepare_features(quotes_df, news_df)
        
        if len(X) < 50:
            return None
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test —Å —Ä–∞–∑–Ω—ã–º random_state –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42 + iteration, shuffle=True
        )
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # –í–∞—Ä–∏–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
        n_estimators = 200 + (iteration % 5) * 50  # 200-400
        max_depth = 10 + (iteration % 10)  # 10-19
        learning_rate = 0.01 + (iteration % 10) * 0.01  # 0.01-0.1
        
        self._update_progress('running', f'training_iter_{iteration}', 
            int((_training_progress['iteration'] / 100) * 100),
            f'üå≤ –ò—Ç–µ—Ä–∞—Ü–∏—è {iteration}: n_est={n_estimators}, depth={max_depth}, lr={learning_rate:.2f}')
        
        # RandomForest
        self.rf_model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=2 + (iteration % 3),
            min_samples_leaf=1 + (iteration % 2),
            max_features='sqrt',
            random_state=42 + iteration,
            n_jobs=-1
        )
        self.rf_model.fit(X_train_scaled, y_train)
        rf_acc = accuracy_score(y_test, self.rf_model.predict(X_test_scaled))
        
        # GradientBoosting
        self.gb_model = GradientBoostingClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth // 2,
            learning_rate=learning_rate,
            min_samples_split=2 + (iteration % 3),
            subsample=0.7 + (iteration % 4) * 0.05,
            random_state=42 + iteration
        )
        self.gb_model.fit(X_train_scaled, y_train)
        gb_acc = accuracy_score(y_test, self.gb_model.predict(X_test_scaled))
        
        # ExtraTrees
        self.et_model = ExtraTreesClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=2 + (iteration % 3),
            random_state=42 + iteration,
            n_jobs=-1
        )
        self.et_model.fit(X_train_scaled, y_train)
        et_acc = accuracy_score(y_test, self.et_model.predict(X_test_scaled))
        
        # Neural Network —Å —Ä–∞–∑–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π
        hidden_layers = [
            (128, 64, 32),
            (256, 128, 64),
            (128, 128, 64, 32),
            (256, 128, 64, 32),
            (512, 256, 128)
        ][iteration % 5]
        
        self.mlp_model = MLPClassifier(
            hidden_layer_sizes=hidden_layers,
            activation='relu',
            solver='adam',
            alpha=0.0001 * (1 + iteration % 10),
            learning_rate='adaptive',
            learning_rate_init=0.001,
            max_iter=500 + iteration * 50,
            early_stopping=True,
            validation_fraction=0.1,
            random_state=42 + iteration
        )
        self.mlp_model.fit(X_train_scaled, y_train)
        mlp_acc = accuracy_score(y_test, self.mlp_model.predict(X_test_scaled))
        
        # AdaBoost
        self.ada_model = AdaBoostClassifier(
            n_estimators=100 + iteration * 10,
            learning_rate=learning_rate,
            random_state=42 + iteration
        )
        self.ada_model.fit(X_train_scaled, y_train)
        ada_acc = accuracy_score(y_test, self.ada_model.predict(X_test_scaled))
        
        # –ê–Ω—Å–∞–º–±–ª—å
        rf_pred = self.rf_model.predict_proba(X_test_scaled)[:, 1]
        gb_pred = self.gb_model.predict_proba(X_test_scaled)[:, 1]
        et_pred = self.et_model.predict_proba(X_test_scaled)[:, 1]
        mlp_pred = self.mlp_model.predict_proba(X_test_scaled)[:, 1]
        ada_pred = self.ada_model.predict_proba(X_test_scaled)[:, 1]
        
        weights = np.array([rf_acc, gb_acc, et_acc, mlp_acc, ada_acc])
        weights = weights / weights.sum()
        
        ensemble_proba = (
            weights[0] * rf_pred + 
            weights[1] * gb_pred + 
            weights[2] * et_pred + 
            weights[3] * mlp_pred + 
            weights[4] * ada_pred
        )
        y_pred = (ensemble_proba > 0.5).astype(int)
        
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        self._update_progress('running', f'results_iter_{iteration}', 
            int((_training_progress['iteration'] / 100) * 100),
            f'üìä Iter {iteration}: RF={rf_acc:.1%} GB={gb_acc:.1%} ET={et_acc:.1%} MLP={mlp_acc:.1%} ADA={ada_acc:.1%} ‚Üí Ensemble={accuracy:.2%}')
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Ä—Å–∏—é
        version_parts = self.model_version.split('.')
        version_parts[-1] = str(int(version_parts[-1]) + 1)
        new_version = '.'.join(version_parts)
        
        cv_scores = cross_val_score(self.rf_model, X_train_scaled, y_train, cv=3)
        
        metrics = TrainingMetrics(
            timestamp=datetime.now().isoformat(),
            accuracy=accuracy,
            precision=precision,
            recall=recall,
            f1=f1,
            training_samples=len(X_train),
            validation_samples=len(X_test),
            feature_count=len(self.feature_names),
            model_version=new_version,
            cross_val_mean=cv_scores.mean(),
            cross_val_std=cv_scores.std()
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ—Å–ª–∏ –ª—É—á—à–µ
        if accuracy > self.best_accuracy:
            self.model_version = new_version
            self.best_accuracy = accuracy
            self._save_model(metrics)
            self._save_metrics_to_db(metrics)
        
        return metrics
    
    def stop_continuous_training(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        global _stop_continuous_training
        _stop_continuous_training = True
        logger.info("Continuous training stop requested")
    
    def get_feature_importance(self) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        if self.rf_model is None:
            return {}
        
        importance = dict(zip(self.feature_names, self.rf_model.feature_importances_))
        return dict(sorted(importance.items(), key=lambda x: x[1], reverse=True))
    
    def predict(self, X: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π.
        
        Returns:
            (predictions, probabilities)
        """
        if self.rf_model is None or self.gb_model is None:
            raise ValueError("Model not trained")
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        for col in self.feature_names:
            if col not in X.columns:
                X[col] = 0
        
        X = X[self.feature_names].fillna(0)
        X_scaled = self.scaler.transform(X)
        
        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        predictions = []
        predictions.append(self.rf_model.predict_proba(X_scaled)[:, 1])
        predictions.append(self.gb_model.predict_proba(X_scaled)[:, 1])
        
        if self.et_model is not None:
            predictions.append(self.et_model.predict_proba(X_scaled)[:, 1])
        if self.mlp_model is not None:
            predictions.append(self.mlp_model.predict_proba(X_scaled)[:, 1])
        if self.ada_model is not None:
            predictions.append(self.ada_model.predict_proba(X_scaled)[:, 1])
        
        # –£—Å—Ä–µ–¥–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        proba = np.mean(predictions, axis=0)
        pred = (proba > 0.5).astype(int)
        
        return pred, proba
    
    def get_metrics_history(self, limit: int = 20) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –º–µ—Ç—Ä–∏–∫ –∏–∑ –ë–î"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT timestamp, accuracy, precision_score, recall, f1_score,
                   training_samples, validation_samples, feature_count, model_version
            FROM model_metrics
            ORDER BY id DESC
            LIMIT ?
        ''', (limit,))
        
        rows = cursor.fetchall()
        conn.close()
        
        return [
            {
                'timestamp': row[0],
                'accuracy': row[1],
                'precision': row[2],
                'recall': row[3],
                'f1': row[4],
                'training_samples': row[5],
                'validation_samples': row[6],
                'feature_count': row[7],
                'model_version': row[8]
            }
            for row in rows
        ]
    
    def get_current_stats(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–∏"""
        return {
            'model_version': self.model_version,
            'best_accuracy': self.best_accuracy,
            'feature_count': len(self.feature_names),
            'is_trained': self.rf_model is not None,
            'feature_importance': self.get_feature_importance()
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
_trainer = None

def get_model_trainer() -> UniversalModelTrainer:
    global _trainer
    if _trainer is None:
        _trainer = UniversalModelTrainer()
    return _trainer

def get_training_progress() -> Dict:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    global _training_progress
    return _training_progress.copy()
